
from fastapi import FastAPI, HTTPException, Query, UploadFile, File, Form
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, StreamingResponse
from pydantic import BaseModel
import json
import os
import sys
import logging
from typing import Optional, List
import asyncio
from contextlib import asynccontextmanager

# Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…Ø³ÛŒØ± src Ø¨Ù‡ sys.path
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src'))

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¯Ø±Ø®ÙˆØ§Ø³Øª
class ChatRequest(BaseModel):
    message: str
    session_id: Optional[str] = None
    temperature: Optional[float] = 0.7
    max_tokens: Optional[int] = 500

class BatchRequest(BaseModel):
    messages: List[str]
    session_id: Optional[str] = None

class ModelLoadRequest(BaseModel):
    model_name: str = "HooshvareLab/bert-base-parsbert-uncased"
    use_local: bool = True

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Ù…Ø¯ÛŒØ±ÛŒØª lifecycle Ø¨Ø±Ù†Ø§Ù…Ù‡"""
    logger.info("ğŸš€ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø¨Ø±Ù†Ø§Ù…Ù‡ FastAPI")
    
    # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ Ø¨Ù‡ ØµÙˆØ±Øª lazy (Ù‡Ù†Ú¯Ø§Ù… Ø§ÙˆÙ„ÛŒÙ† Ø¯Ø±Ø®ÙˆØ§Ø³Øª)
    app.state.model_loaded = False
    app.state.nlp_processor = None
    
    yield
    
    # Cleanup
    logger.info("ğŸ”´ Ø®Ø§Ù…ÙˆØ´ Ú©Ø±Ø¯Ù† Ø¨Ø±Ù†Ø§Ù…Ù‡")
    if app.state.nlp_processor:
        # Ø°Ø®ÛŒØ±Ù‡ Ø­Ø§Ù„Øª Ù…Ø¯Ù„
        try:
            app.state.nlp_processor.save_model_locally()
        except:
            pass

# Ø§ÛŒØ¬Ø§Ø¯ Ø¨Ø±Ù†Ø§Ù…Ù‡ FastAPI
app = FastAPI(
    title="natiq-ultimate API",
    description="API Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø²Ø¨Ø§Ù† ÙØ§Ø±Ø³ÛŒ",
    version="1.0.0",
    lifespan=lifespan
)

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…ÙˆÙ‚Øª Ø¨Ø±Ø§ÛŒ sessions
app.state.sessions = {}

def load_nlp_processor():
    """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´Ú¯Ø± NLP (lazy loading)"""
    if not app.state.model_loaded or app.state.nlp_processor is None:
        try:
            from src.core.nlp_processor import NLPProcessor
            from src.config import settings
            
            logger.info("Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ NLP...")
            
            # ØªÙ†Ø¸ÛŒÙ… Ù…Ø³ÛŒØ± Ú©Ø´ Ø¨Ø±Ø§ÛŒ Vercel
            cache_dir = os.getenv("MODEL_CACHE_DIR", "/tmp/natiq-models")
            settings["model"].LOCAL_MODEL_PATH = cache_dir
            
            app.state.nlp_processor = NLPProcessor()
            app.state.model_loaded = True
            logger.info("âœ… Ù…Ø¯Ù„ NLP Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯")
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„: {e}")
            raise HTTPException(status_code=500, detail=f"Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„: {str(e)}")
    
    return app.state.nlp_processor

# Routes
@app.get("/")
async def root():
    """ØµÙØ­Ù‡ Ø§ØµÙ„ÛŒ"""
    return {
        "message": "Ø®ÙˆØ´ Ø¢Ù…Ø¯ÛŒØ¯ Ø¨Ù‡ natiq-ultimate API",
        "version": "1.0.0",
        "status": "ÙØ¹Ø§Ù„",
        "endpoints": {
            "chat": "/api/chat",
            "batch": "/api/batch",
            "health": "/api/health",
            "load_model": "/api/load-model"
        }
    }

@app.get("/api/health")
async def health_check():
    """Ø¨Ø±Ø±Ø³ÛŒ Ø³Ù„Ø§Ù…Øª API"""
    try:
        processor = load_nlp_processor()
        return {
            "status": "healthy",
            "model_loaded": app.state.model_loaded,
            "environment": os.getenv("VERCEL_ENV", "development"),
            "python_version": sys.version
        }
    except Exception as e:
        return {
            "status": "unhealthy",
            "error": str(e)
        }

@app.post("/api/chat")
async def chat_endpoint(request: ChatRequest):
    """Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù¾ÛŒØ§Ù… Ù…ØªÙ†ÛŒ"""
    try:
        processor = load_nlp_processor()
        
        logger.info(f"Ø¯Ø±ÛŒØ§ÙØª Ù¾ÛŒØ§Ù…: {request.message[:50]}...")
        
        # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ØªÙ†
        result = processor.process(request.message)
        
        # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± session (Ø§Ú¯Ø± session_id ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯)
        if request.session_id:
            if request.session_id not in app.state.sessions:
                app.state.sessions[request.session_id] = []
            app.state.sessions[request.session_id].append({
                "message": request.message,
                "response": result,
                "timestamp": asyncio.get_event_loop().time()
            })
        
        return {
            "success": True,
            "response": result.get("fallback_response") if "error" in result else result,
            "session_id": request.session_id,
            "processing_time": result.get("processing_time", 0)
        }
        
    except Exception as e:
        logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù¾ÛŒØ§Ù…: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/batch")
async def batch_endpoint(request: BatchRequest):
    """Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ Ù…ØªÙˆÙ†"""
    try:
        processor = load_nlp_processor()
        
        logger.info(f"Ø¯Ø±ÛŒØ§ÙØª {len(request.messages)} Ù¾ÛŒØ§Ù… Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ")
        
        results = []
        for i, message in enumerate(request.messages):
            result = processor.process(message)
            results.append({
                "index": i,
                "message": message,
                "result": result
            })
        
        return {
            "success": True,
            "total": len(results),
            "results": results,
            "session_id": request.session_id
        }
        
    except Exception as e:
        logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/load-model")
async def load_model_endpoint(request: ModelLoadRequest):
    """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ Ø¬Ø¯ÛŒØ¯"""
    try:
        from src.core.nlp_processor import NLPProcessor
        
        logger.info(f"Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„: {request.model_name}")
        
        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ Ø¬Ø¯ÛŒØ¯
        processor = NLPProcessor(model_name=request.model_name)
        app.state.nlp_processor = processor
        app.state.model_loaded = True
        
        return {
            "success": True,
            "message": f"Ù…Ø¯Ù„ {request.model_name} Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯",
            "model_name": request.model_name
        }
        
    except Exception as e:
        logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/session/{session_id}")
async def get_session(session_id: str):
    """Ø¯Ø±ÛŒØ§ÙØª ØªØ§Ø±ÛŒØ®Ú†Ù‡ session"""
    if session_id in app.state.sessions:
        return {
            "success": True,
            "session_id": session_id,
            "messages": app.state.sessions[session_id],
            "count": len(app.state.sessions[session_id])
        }
    else:
        return {
            "success": False,
            "message": "Session not found"
        }

# Ø¨Ø±Ø§ÛŒ Vercel Serverless
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
