"""
Ø³ÛŒØ³ØªÙ… Ø¹ØµØ¨ÛŒ natiq-ultimate
Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø²Ø¨Ø§Ù† Ø·Ø¨ÛŒØ¹ÛŒ Ùˆ ØªØ­Ù„ÛŒÙ„ Ù…Ø¹Ù†Ø§ÛŒÛŒ
"""
import re
import math
import random
import hashlib
from datetime import datetime

class NeuralSystem:
    """Ø³ÛŒØ³ØªÙ… Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¹ØµØ¨ÛŒ"""
    
    def __init__(self):
        self.word_vectors = self._initialize_embeddings()
        self.intent_patterns = self._initialize_intents()
        self.entity_patterns = self._initialize_entities()
        self.cache = {}
        print("ðŸ§  Neural system initialized")
    
    def _initialize_embeddings(self):
        """Ø§ÛŒØ¬Ø§Ø¯ Ø¨Ø±Ø¯Ø§Ø±Ù‡Ø§ÛŒ Ú©Ù„Ù…Ø§Øª (Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø´Ø¯Ù‡)"""
        words = [
            "Ù‡ÙˆØ´", "Ù…ØµÙ†ÙˆØ¹ÛŒ", "ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ", "Ù…Ø§Ø´ÛŒÙ†", "Ø¯Ø§Ø¯Ù‡", "Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…",
            "Ø´Ø¨Ú©Ù‡", "Ø¹ØµØ¨ÛŒ", "Ø¹Ù…ÛŒÙ‚", "Ù¾Ø§ÛŒØªÙˆÙ†", "Ø¨Ø±Ù†Ø§Ù…Ù‡", "Ù†ÙˆÛŒØ³ÛŒ",
            "ØªØ­Ù„ÛŒÙ„", "Ù¾Ø±Ø¯Ø§Ø²Ø´", "Ø²Ø¨Ø§Ù†", "Ø·Ø¨ÛŒØ¹ÛŒ", "Ø¨ÛŒÙ†Ø§ÛŒÛŒ", "Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ±",
            "Ø±Ø¨Ø§Øª", "Ø±Ø¨Ø§ØªÛŒÚ©", "Ø¯Ø§Ø¯Ù‡â€ŒÚ©Ø§ÙˆÛŒ", "Ú©Ø§ÙˆØ´", "Ø§Ù„Ú¯Ùˆ", "Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ",
            "Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ", "Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ", "Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ", "Ø¨Ø§Ø²Ú¯Ø´ØªÛŒ", "Ú©Ø§Ù†ÙˆÙ„ÙˆØ´Ù†"
        ]
        
        vectors = {}
        for word in words:
            # Ø¨Ø±Ø¯Ø§Ø± 10 Ø¨Ø¹Ø¯ÛŒ Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø´Ø¯Ù‡
            vector = [random.random() for _ in range(10)]
            norm = math.sqrt(sum(x*x for x in vector))
            if norm > 0:
                vector = [x/norm for x in vector]
            vectors[word] = vector
        
        return vectors
    
    def _initialize_intents(self):
        """Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ù‡Ø¯Ù Ø³ÙˆØ§Ù„"""
        return {
            "definition": {
                "patterns": ["Ú†ÛŒØ³Øª", "Ú†ÛŒÙ‡", "ØªØ¹Ø±ÛŒÙ", "Ù…Ù†Ø¸ÙˆØ±", "Ù…Ø¹Ù†ÛŒ", "Ú†Ù‡"],
                "keywords": ["ØªØ¹Ø±ÛŒÙ", "Ù…Ø¹Ù†ÛŒ", "Ù…ÙÙ‡ÙˆÙ…"],
                "weight": 1.0
            },
            "comparison": {
                "patterns": ["ØªÙØ§ÙˆØª", "ÙØ±Ù‚", "Ù…Ù‚Ø§ÛŒØ³Ù‡", "Ø§Ø®ØªÙ„Ø§Ù", "Ú©Ø¯Ø§Ù… Ø¨Ù‡ØªØ±"],
                "keywords": ["Ù…Ù‚Ø§ÛŒØ³Ù‡", "ØªÙØ§ÙˆØª", "ÙØ±Ù‚"],
                "weight": 0.9
            },
            "causal": {
                "patterns": ["Ú†Ø±Ø§", "Ø¹Ù„Øª", "Ø¯Ù„ÛŒÙ„", "Ú†Ø±Ø§ÛŒÛŒ", "Ø³Ø¨Ø¨", "Ú†Ú¯ÙˆÙ†Ù‡ Ø§ØªÙØ§Ù‚"],
                "keywords": ["Ø¹Ù„Øª", "Ø¯Ù„ÛŒÙ„", "Ú†Ø±Ø§"],
                "weight": 0.8
            },
            "howto": {
                "patterns": ["Ú†Ú¯ÙˆÙ†Ù‡", "Ú†Ø·ÙˆØ±", "Ø±ÙˆØ´", "Ø·Ø±ÛŒÙ‚", "Ù…Ø±Ø§Ø­Ù„", "Ú†Ú©Ø§Ø± Ú©Ù†Ù…"],
                "keywords": ["Ú†Ú¯ÙˆÙ†Ù‡", "Ø±ÙˆØ´", "Ø·Ø±ÛŒÙ‚"],
                "weight": 0.85
            },
            "application": {
                "patterns": ["Ú©Ø§Ø±Ø¨Ø±Ø¯", "Ø§Ø³ØªÙØ§Ø¯Ù‡", "ÙÙˆØ§ÛŒØ¯", "Ù…Ø²Ø§ÛŒØ§", "Ù…Ù†Ø§ÙØ¹", "Ú©Ø¬Ø§ Ø¨Ú©Ø§Ø±"],
                "keywords": ["Ú©Ø§Ø±Ø¨Ø±Ø¯", "Ø§Ø³ØªÙØ§Ø¯Ù‡", "Ù…Ø²Ø§ÛŒØ§"],
                "weight": 0.75
            },
            "component": {
                "patterns": ["Ø§Ø¬Ø²Ø§", "Ù‚Ø³Ù…Øªâ€ŒÙ‡Ø§", "Ù…ÙˆÙ„ÙÙ‡â€ŒÙ‡Ø§", "Ø¨Ø®Ø´â€ŒÙ‡Ø§", "Ø¹Ù†Ø§ØµØ±"],
                "keywords": ["Ø§Ø¬Ø²Ø§", "Ø¨Ø®Ø´", "Ù…ÙˆÙ„ÙÙ‡"],
                "weight": 0.7
            },
            "example": {
                "patterns": ["Ù…Ø«Ø§Ù„", "Ù†Ù…ÙˆÙ†Ù‡", "Ù…ÙˆØ±Ø¯", "Ú©Ø§Ø±Ø¨Ø±Ø¯ Ø¹Ù…Ù„ÛŒ"],
                "keywords": ["Ù…Ø«Ø§Ù„", "Ù†Ù…ÙˆÙ†Ù‡"],
                "weight": 0.65
            }
        }
    
    def _initialize_entities(self):
        """Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ù…ÙˆØ¬ÙˆØ¯ÛŒØªâ€ŒÙ‡Ø§"""
        return {
            "CONCEPT": ["Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ", "ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø§Ø´ÛŒÙ†", "Ø´Ø¨Ú©Ù‡ Ø¹ØµØ¨ÛŒ", "ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø¹Ù…ÛŒÙ‚",
                       "Ù¾Ø§ÛŒØªÙˆÙ†", "Ø¯Ø§Ø¯Ù‡ Ú©Ø§ÙˆÛŒ", "Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…", "Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø²Ø¨Ø§Ù† Ø·Ø¨ÛŒØ¹ÛŒ",
                       "Ø¨ÛŒÙ†Ø§ÛŒÛŒ Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ±", "Ø±Ø¨Ø§ØªÛŒÚ©"],
            "TECHNOLOGY": ["ØªÙ†Ø³ÙˆØ±ÙÙ„Ùˆ", "Ù¾Ø§ÛŒØªÙˆØ±Ú†", "keras", "scikit-learn", "numpy", "pandas"],
            "ALGORITHM": ["Ø¯Ø±Ø®Øª ØªØµÙ…ÛŒÙ…", "Ù…Ø§Ø´ÛŒÙ† Ø¨Ø±Ø¯Ø§Ø± Ù¾Ø´ØªÛŒØ¨Ø§Ù†", "Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ", "Ø±Ú¯Ø±Ø³ÛŒÙˆÙ†",
                         "Ø´Ø¨Ú©Ù‡ Ú©Ø§Ù†ÙˆÙ„ÙˆØ´Ù†", "Ø´Ø¨Ú©Ù‡ Ø¨Ø§Ø²Ú¯Ø´ØªÛŒ", "Ù¾Ø±Ø³Ù¾ØªØ±ÙˆÙ†"],
            "PROCESS": ["Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ", "Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ", "ØªØ´Ø®ÛŒØµ", "ØªØ­Ù„ÛŒÙ„", "Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ"]
        }
    
    def analyze(self, text):
        """ØªØ­Ù„ÛŒÙ„ Ú©Ø§Ù…Ù„ Ù…ØªÙ† Ø¨Ø§ Ø³ÛŒØ³ØªÙ… Ø¹ØµØ¨ÛŒ"""
        text_lower = text.lower()
        
        # 1. ØªØ´Ø®ÛŒØµ Ù‡Ø¯Ù
        intent = self._detect_intent(text_lower)
        
        # 2. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÙˆØ¬ÙˆØ¯ÛŒØªâ€ŒÙ‡Ø§
        entities = self._extract_entities(text)
        
        # 3. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÙØ§Ù‡ÛŒÙ…
        concepts = self._extract_concepts(text)
        
        # 4. Ù…Ø­Ø§Ø³Ø¨Ù‡ embedding
        embedding = self._get_sentence_embedding(text)
        
        # 5. Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ú©Ù„ÛŒ
        confidence = self._calculate_confidence(intent, entities, concepts)
        
        return {
            "text": text,
            "intent": intent["type"],
            "intent_confidence": intent["confidence"],
            "intent_details": intent,
            "entities": entities,
            "concepts": concepts,
            "embedding": embedding[:5],  # ÙÙ‚Ø· 5 Ø¨Ø¹Ø¯ Ø§ÙˆÙ„
            "confidence": confidence,
            "word_count": len(text.split()),
            "has_question": "ØŸ" in text or "?" in text,
            "timestamp": datetime.now().isoformat(),
            "neural_version": "1.0.0"
        }
    
    def _detect_intent(self, text):
        """ØªØ´Ø®ÛŒØµ Ù‡Ø¯Ù Ø³ÙˆØ§Ù„"""
        scores = {}
        
        for intent_type, intent_data in self.intent_patterns.items():
            score = 0
            
            # Ø¨Ø±Ø±Ø³ÛŒ Ø§Ù„Ú¯ÙˆÙ‡Ø§
            for pattern in intent_data["patterns"]:
                if pattern in text:
                    score += 1
            
            # Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ
            for keyword in intent_data["keywords"]:
                if keyword in text:
                    score += 2
            
            # Ø§Ø¹Ù…Ø§Ù„ ÙˆØ²Ù†
            scores[intent_type] = (score * intent_data["weight"]) / 10
        
        # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø§Ù…ØªÛŒØ§Ø²Ø§Øª
        max_score = max(scores.values()) if scores else 0
        if max_score > 0:
            for intent_type in scores:
                scores[intent_type] = min(scores[intent_type] / max_score, 1.0)
        
        # Ø§Ù†ØªØ®Ø§Ø¨ Ù‡Ø¯Ù Ø§ØµÙ„ÛŒ
        primary_intent = max(scores.items(), key=lambda x: x[1]) if scores else ("general", 0.5)
        
        return {
            "type": primary_intent[0],
            "confidence": round(primary_intent[1], 2),
            "all_scores": scores,
            "details": self.intent_patterns.get(primary_intent[0], {})
        }
    
    def _extract_entities(self, text):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÙˆØ¬ÙˆØ¯ÛŒØªâ€ŒÙ‡Ø§ Ø§Ø² Ù…ØªÙ†"""
        entities = []
        text_lower = text.lower()
        
        for entity_type, entity_list in self.entity_patterns.items():
            for entity in entity_list:
                if entity.lower() in text_lower:
                    # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ù…ÙˆÙ‚Ø¹ÛŒØª
                    start = text_lower.find(entity.lower())
                    end = start + len(entity)
                    
                    entities.append({
                        "entity": entity,
                        "type": entity_type,
                        "start": start,
                        "end": end,
                        "confidence": random.uniform(0.7, 0.95)  # Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø´Ø¯Ù‡
                    })
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ø¯ÛŒÚ¯Ø±
        words = text.split()
        for i, word in enumerate(words):
            if len(word) > 3 and word in self.word_vectors:
                entities.append({
                    "entity": word,
                    "type": "KEYWORD",
                    "start": i,
                    "end": i + 1,
                    "confidence": 0.6
                })
        
        return entities
    
    def _extract_concepts(self, text):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÙØ§Ù‡ÛŒÙ… Ø§Ø² Ù…ØªÙ†"""
        concepts = []
        text_lower = text.lower()
        
        # Ù„ÛŒØ³Øª Ù…ÙØ§Ù‡ÛŒÙ… Ø´Ù†Ø§Ø®ØªÙ‡ Ø´Ø¯Ù‡
        known_concepts = [
            "Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ", "ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø§Ø´ÛŒÙ†", "Ø´Ø¨Ú©Ù‡ Ø¹ØµØ¨ÛŒ", "ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø¹Ù…ÛŒÙ‚",
            "Ù¾Ø§ÛŒØªÙˆÙ†", "Ø¯Ø§Ø¯Ù‡ Ú©Ø§ÙˆÛŒ", "Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…", "Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø²Ø¨Ø§Ù† Ø·Ø¨ÛŒØ¹ÛŒ",
            "Ø¨ÛŒÙ†Ø§ÛŒÛŒ Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ±", "Ø±Ø¨Ø§ØªÛŒÚ©", "ØªÙ†Ø³ÙˆØ±ÙÙ„Ùˆ", "Ù¾Ø§ÛŒØªÙˆØ±Ú†"
        ]
        
        for concept in known_concepts:
            if concept.lower() in text_lower:
                concepts.append(concept)
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù„Ù…Ø§Øª ØªØ®ØµØµÛŒ
        for word in text.split():
            if word in self.word_vectors and len(word) > 2:
                # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø¨Ø§Ù‡Øª Ø¨Ø§ Ù…ÙØ§Ù‡ÛŒÙ… Ø´Ù†Ø§Ø®ØªÙ‡ Ø´Ø¯Ù‡
                similarity = self._calculate_word_similarity(word, known_concepts)
                if similarity > 0.5:
                    concepts.append(word)
        
        # Ø­Ø°Ù ØªÚ©Ø±Ø§Ø±ÛŒâ€ŒÙ‡Ø§
        return list(dict.fromkeys(concepts))
    
    def _get_sentence_embedding(self, text):
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ embedding Ø¬Ù…Ù„Ù‡"""
        words = text.split()
        vectors = []
        
        for word in words:
            if word in self.word_vectors:
                vectors.append(self.word_vectors[word])
            else:
                # Ø¨Ø±Ø¯Ø§Ø± Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ø¨Ø±Ø§ÛŒ Ú©Ù„Ù…Ø§Øª Ù†Ø§Ø´Ù†Ø§Ø®ØªÙ‡
                vec = [random.random() for _ in range(10)]
                norm = math.sqrt(sum(x*x for x in vec))
                if norm > 0:
                    vec = [x/norm for x in vec]
                vectors.append(vec)
        
        if vectors:
            # Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø¨Ø±Ø¯Ø§Ø±Ù‡Ø§
            result = [0.0] * 10
            for vec in vectors:
                for i in range(10):
                    result[i] += vec[i]
            return [x/len(vectors) for x in result]
        
        return [0.0] * 10
    
    def _calculate_word_similarity(self, word1, word2_list):
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø¨Ø§Ù‡Øª Ø¨ÛŒÙ† Ú©Ù„Ù…Ù‡ Ùˆ Ù„ÛŒØ³Øª Ú©Ù„Ù…Ø§Øª"""
        if word1 not in self.word_vectors:
            return 0.0
        
        max_similarity = 0.0
        vec1 = self.word_vectors[word1]
        
        for word2 in word2_list:
            for w in word2.split():
                if w in self.word_vectors:
                    vec2 = self.word_vectors[w]
                    similarity = self._cosine_similarity(vec1, vec2)
                    max_similarity = max(max_similarity, similarity)
        
        return max_similarity
    
    def _cosine_similarity(self, vec1, vec2):
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø¨Ø§Ù‡Øª Ú©Ø³ÛŒÙ†ÙˆØ³ÛŒ"""
        dot_product = sum(a*b for a,b in zip(vec1, vec2))
        norm1 = math.sqrt(sum(a*a for a in vec1))
        norm2 = math.sqrt(sum(b*b for b in vec2))
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
        
        return dot_product / (norm1 * norm2)
    
    def _calculate_confidence(self, intent, entities, concepts):
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ú©Ù„ÛŒ ØªØ­Ù„ÛŒÙ„"""
        base_confidence = intent["confidence"]
        
        # Ø§ÙØ²Ø§ÛŒØ´ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…ÙˆØ¬ÙˆØ¯ÛŒØªâ€ŒÙ‡Ø§
        if entities:
            base_confidence += min(len(entities) * 0.05, 0.2)
        
        # Ø§ÙØ²Ø§ÛŒØ´ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…ÙØ§Ù‡ÛŒÙ…
        if concepts:
            base_confidence += min(len(concepts) * 0.1, 0.3)
        
        # Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±Ø¯Ù† Ø¨Ù‡ Ø¨Ø§Ø²Ù‡ [0, 1]
        return min(max(base_confidence, 0.1), 0.95)
    
    def semantic_search(self, query, documents, top_k=3):
        """Ø¬Ø³ØªØ¬ÙˆÛŒ Ù…Ø¹Ù†Ø§ÛŒÛŒ (Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø´Ø¯Ù‡)"""
        # Ø¯Ø± Ù†Ø³Ø®Ù‡ ÙˆØ§Ù‚Ø¹ÛŒØŒ Ø§ÛŒÙ† Ø¨Ø®Ø´ Ø¨Ø§ Ù…Ø¯Ù„ embedding Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…ÛŒâ€ŒØ´ÙˆØ¯
        results = []
        
        for i, doc in enumerate(documents[:10]):  # Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±Ø¯Ù† Ø¨Ø±Ø§ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯
            # Ø´Ø¨Ø§Ù‡Øª Ø³Ø§Ø¯Ù‡ (Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø´Ø¯Ù‡)
            similarity = random.uniform(0.1, 0.9)
            
            results.append({
                "document": doc[:100] + "..." if len(doc) > 100 else doc,
                "similarity": round(similarity, 2),
                "index": i
            })
        
        # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø´Ø¨Ø§Ù‡Øª
        results.sort(key=lambda x: x["similarity"], reverse=True)
        return results[:top_k]
    
    def cache_key(self, text):
        """Ø§ÛŒØ¬Ø§Ø¯ Ú©Ù„ÛŒØ¯ Ú©Ø´"""
        return hashlib.md5(text.encode()).hexdigest()[:16]
