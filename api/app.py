#!/usr/bin/env python3
"""
ÙØ§ÛŒÙ„ Ø§ØµÙ„ÛŒ FastAPI Ø¨Ø±Ø§ÛŒ Ù¾Ø±ÙˆÚ˜Ù‡ natiq-ultimate
Ø¨Ø§ Ù‚Ø§Ø¨Ù„ÛŒØª Ø§ØªØµØ§Ù„ Ø¨Ù‡ OpenAI API
"""

from fastapi import FastAPI, HTTPException, Request
from fastapi.responses import JSONResponse, FileResponse, HTMLResponse, RedirectResponse
from fastapi.middleware.cors import CORSMiddleware
from openai import OpenAI
import logging
import os
import json
import asyncio
from typing import Optional

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù„Ø§Ú¯â€ŒÚ¯ÛŒØ±ÛŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ØªÙ†Ø¸ÛŒÙ… OpenAI API
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    logger.warning("âš ï¸ OPENAI_API_KEY ÛŒØ§ÙØª Ù†Ø´Ø¯! Ø§Ø² Ù…Ø¯Ù„ Ù…Ø­Ù„ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯.")
    client = None
else:
    client = OpenAI(api_key=OPENAI_API_KEY)
    logger.info("âœ… OpenAI API Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ Ø´Ø¯")

# Ø§ÛŒØ¬Ø§Ø¯ Ù†Ù…ÙˆÙ†Ù‡ FastAPI
app = FastAPI(
    title="Natiq Ultimate AI",
    description="Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ ÙØ§Ø±Ø³ÛŒ Ø¨Ø§ Ù‚Ø§Ø¨Ù„ÛŒØª Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ØªÙ†",
    version="1.5.0",
    docs_url=None,
    redoc_url=None,
    openapi_url="/api/openapi.json"
)

# ØªÙ†Ø¸ÛŒÙ… CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Ø³ÛŒØ³ØªÙ… Ø­Ø§ÙØ¸Ù‡ Ù…Ú©Ø§Ù„Ù…Ù‡
conversation_memory = {}

def get_or_create_memory(session_id: str):
    """Ø§ÛŒØ¬Ø§Ø¯ ÛŒØ§ Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø­Ø§ÙØ¸Ù‡ Ù…Ú©Ø§Ù„Ù…Ù‡"""
    if session_id not in conversation_memory:
        conversation_memory[session_id] = {
            "history": [],
            "context": "Ø´Ù…Ø§ ÛŒÚ© Ø¯Ø³ØªÛŒØ§Ø± Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ ÙØ§Ø±Ø³ÛŒ Ø¨Ù‡ Ù†Ø§Ù… 'Ù†Ø§Ø·Ù‚ Ø§ÙˆÙ„ØªÛŒÙ…ÛŒØª' Ù‡Ø³ØªÛŒØ¯. Ø´Ù…Ø§ Ù…Ù‡Ø±Ø¨Ø§Ù†ØŒ Ù…ÙÛŒØ¯ Ùˆ Ø¯Ù‚ÛŒÙ‚ Ù‡Ø³ØªÛŒØ¯. Ø¨Ù‡ Ø³ÙˆØ§Ù„Ø§Øª Ø¨Ù‡ Ø²Ø¨Ø§Ù† ÙØ§Ø±Ø³ÛŒ Ù¾Ø§Ø³Ø® Ù…ÛŒâ€ŒØ¯Ù‡ÛŒØ¯.",
            "created_at": asyncio.get_event_loop().time()
        }
    return conversation_memory[session_id]

def cleanup_old_memory(max_age_hours=24):
    """Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø­Ø§ÙØ¸Ù‡ Ù‚Ø¯ÛŒÙ…ÛŒ"""
    current_time = asyncio.get_event_loop().time()
    to_delete = []
    
    for session_id, memory in conversation_memory.items():
        age_hours = (current_time - memory["created_at"]) / 3600
        if age_hours > max_age_hours:
            to_delete.append(session_id)
    
    for session_id in to_delete:
        del conversation_memory[session_id]
    if to_delete:
        logger.info(f"ğŸ§¹ {len(to_delete)} Ù…Ú©Ø§Ù„Ù…Ù‡ Ù‚Ø¯ÛŒÙ…ÛŒ Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø´Ø¯")

async def call_openai(prompt: str, session_id: str = "default") -> str:
    """ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ OpenAI API"""
    
    # Ø§Ú¯Ø± API Key Ù†Ø¨ÙˆØ¯ØŒ Ø§Ø² Ù…Ø¯Ù„ Ù…Ø­Ù„ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†
    if not client:
        return await get_fallback_response(prompt, session_id)
    
    memory = get_or_create_memory(session_id)
    
    # Ø³Ø§Ø®Øª ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ù…Ú©Ø§Ù„Ù…Ù‡
    messages = [
        {"role": "system", "content": memory["context"]},
    ]
    
    # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ù…Ú©Ø§Ù„Ù…Ù‡ (Ø¢Ø®Ø±ÛŒÙ† Û±Û° Ù¾ÛŒØ§Ù…)
    for msg in memory["history"][-10:]:
        messages.append({"role": msg["role"], "content": msg["content"]})
    
    messages.append({"role": "user", "content": prompt})
    
    try:
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=messages,
            temperature=0.7,
            max_tokens=500,
            stream=False
        )
        
        ai_response = response.choices[0].message.content
        
        # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Ø­Ø§ÙØ¸Ù‡
        memory["history"].append({"role": "user", "content": prompt})
        memory["history"].append({"role": "assistant", "content": ai_response})
        
        # Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±Ø¯Ù† Ø§Ù†Ø¯Ø§Ø²Ù‡ ØªØ§Ø±ÛŒØ®Ú†Ù‡
        if len(memory["history"]) > 20:
            memory["history"] = memory["history"][-20:]
        
        return ai_response
        
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± OpenAI API: {str(e)}")
        return await get_fallback_response(prompt, session_id)

async def get_fallback_response(prompt: str, session_id: str) -> str:
    """Ù¾Ø§Ø³Ø® Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ø¯Ø± ØµÙˆØ±Øª Ø¹Ø¯Ù… Ø§ØªØµØ§Ù„ Ø¨Ù‡ OpenAI"""
    
    memory = get_or_create_memory(session_id)
    
    # Ø¢Ù†Ø§Ù„ÛŒØ² Ø³ÙˆØ§Ù„
    prompt_lower = prompt.lower()
    
    # Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ†
    responses = {
        "Ø³Ù„Ø§Ù…": "Ø³Ù„Ø§Ù…! ğŸ‘‹ Ø¨Ù‡ Ù†Ø§Ø·Ù‚ Ø§ÙˆÙ„ØªÛŒÙ…ÛŒØª Ø®ÙˆØ´ Ø¢Ù…Ø¯ÛŒØ¯. Ú†Ø·ÙˆØ± Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ù… Ú©Ù…Ú©ØªØ§Ù† Ú©Ù†Ù…ØŸ",
        "Ø®Ø¯Ø§Ø­Ø§ÙØ¸": "Ø®Ø¯Ø§Ø­Ø§ÙØ¸! ğŸ‘‹ Ø§Ø² ØµØ­Ø¨Øª Ø¨Ø§ Ø´Ù…Ø§ Ø®ÙˆØ´Ø­Ø§Ù„ Ø´Ø¯Ù…. Ø§Ú¯Ø± Ø³ÙˆØ§Ù„ Ø¯ÛŒÚ¯Ø±ÛŒ Ø¯Ø§Ø±ÛŒØ¯ Ø¯Ø± Ø®Ø¯Ù…ØªÙ….",
        "Ú†Ø·ÙˆØ±ÛŒ": "Ù…Ù† Ø®ÙˆØ¨Ù… Ù…Ù…Ù†ÙˆÙ†! ğŸ˜Š Ø´Ù…Ø§ Ú†Ø·ÙˆØ±ÛŒØ¯ØŸ",
        "Ø§Ø³Ù…Øª Ú†ÛŒÙ‡": "Ù…Ù† Ù†Ø§Ø·Ù‚ Ø§ÙˆÙ„ØªÛŒÙ…ÛŒØª Ù‡Ø³ØªÙ…ØŒ ÛŒÚ© Ø¯Ø³ØªÛŒØ§Ø± Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ ÙØ§Ø±Ø³ÛŒ.",
        "Ú©Ù…Ú©": "Ø­ØªÙ…Ø§Ù‹! Ø¯Ø± Ú†Ù‡ Ø²Ù…ÛŒÙ†Ù‡â€ŒØ§ÛŒ Ù…ÛŒâ€ŒØªÙˆÙ†Ù… Ú©Ù…Ú© Ú©Ù†Ù…ØŸ",
        "Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ": "Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ø´Ø§Ø®Ù‡â€ŒØ§ÛŒ Ø§Ø² Ø¹Ù„ÙˆÙ… Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ± Ø§Ø³Øª Ú©Ù‡ Ø¨Ù‡ Ø§ÛŒØ¬Ø§Ø¯ Ù…Ø§Ø´ÛŒÙ†â€ŒÙ‡Ø§ÛŒÛŒ Ù…ÛŒâ€ŒÙ¾Ø±Ø¯Ø§Ø²Ø¯ Ú©Ù‡ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ù†Ø¯ Ù…Ø§Ù†Ù†Ø¯ Ø§Ù†Ø³Ø§Ù† ÙÚ©Ø± Ú©Ù†Ù†Ø¯ Ùˆ ÛŒØ§Ø¯ Ø¨Ú¯ÛŒØ±Ù†Ø¯.",
        "openai": "Ø¨Ù„Ù‡ØŒ Ù…Ù† Ø§Ø² OpenAI Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ù…. Ø§Ú¯Ø± API Key ØªÙ†Ø¸ÛŒÙ… Ø´Ø¯Ù‡ Ø¨Ø§Ø´Ø¯ØŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ù… Ø§Ø² Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ GPT Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†Ù….",
        "ØªØ´Ú©Ø±": "Ø®ÙˆØ§Ù‡Ø´ Ù…ÛŒâ€ŒÚ©Ù†Ù…! ğŸ˜Š Ù‡Ù…ÛŒØ´Ù‡ Ø¯Ø± Ø®Ø¯Ù…Øª Ø´Ù…Ø§ Ù‡Ø³ØªÙ….",
        "Ø²Ù…Ø§Ù†": f"Ø²Ù…Ø§Ù† Ø³Ø±ÙˆØ±: {__import__('datetime').datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
    }
    
    # Ø¬Ø³ØªØ¬ÙˆÛŒ Ù¾Ø§Ø³Ø® Ù…Ù†Ø§Ø³Ø¨
    for keyword, response in responses.items():
        if keyword in prompt_lower:
            return response
    
    # Ø§Ú¯Ø± Ø³ÙˆØ§Ù„ ØªÚ©Ø±Ø§Ø±ÛŒ Ø§Ø³Øª
    if len(memory["history"]) > 0:
        last_q = memory["history"][-1]["content"].lower() if memory["history"][-1]["role"] == "user" else ""
        if prompt_lower == last_q:
            return "Ø¨Ù‡ Ù†Ø¸Ø± Ù…ÛŒâ€ŒØ±Ø³Ø¯ Ù‡Ù…ÛŒÙ† Ø³ÙˆØ§Ù„ Ø±Ø§ Ù¾Ø±Ø³ÛŒØ¯Ù‡â€ŒØ§ÛŒØ¯! Ø¢ÛŒØ§ Ù¾Ø§Ø³Ø® Ù…Ù† Ú©Ø§ÙÛŒ Ù†Ø¨ÙˆØ¯ØŸ"
    
    # Ù¾Ø§Ø³Ø® Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ù‡ÙˆØ´Ù…Ù†Ø¯
    if "Ú†ÛŒ" in prompt_lower and "ØŸ" in prompt:
        return f"Ø´Ù…Ø§ Ù¾Ø±Ø³ÛŒØ¯ÛŒØ¯: '{prompt}'. Ù…Ù† ÛŒÚ© Ø¯Ø³ØªÛŒØ§Ø± Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ù‡Ø³ØªÙ… Ùˆ Ø³Ø¹ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ù… Ø¨Ù‡ØªØ±ÛŒÙ† Ù¾Ø§Ø³Ø® Ø±Ø§ Ø¨Ø¯Ù‡Ù…!"
    
    return "Ù…ØªÙˆØ¬Ù‡ Ø³ÙˆØ§Ù„ Ø´Ù…Ø§ Ø´Ø¯Ù…! Ø¯Ø± Ø­Ø§Ù„ Ø­Ø§Ø¶Ø± Ø§ØªØµØ§Ù„ Ø¨Ù‡ OpenAI Ø¨Ø±Ù‚Ø±Ø§Ø± Ù†ÛŒØ³Øª. Ø¨Ø±Ø§ÛŒ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡â€ŒØªØ± Ù„Ø·ÙØ§Ù‹ API Key Ø±Ø§ ØªÙ†Ø¸ÛŒÙ… Ú©Ù†ÛŒØ¯."

# ==================== Ù…Ø³ÛŒØ±Ù‡Ø§ÛŒ API ====================

@app.get("/")
async def root():
    """ØµÙØ­Ù‡ Ø§ØµÙ„ÛŒ"""
    return FileResponse("public/index.html")

@app.get("/api/")
async def api_root():
    """Ø§Ø·Ù„Ø§Ø¹Ø§Øª API"""
    has_openai = bool(client)
    return {
        "message": "Ø¨Ù‡ Ù†Ø§Ø·Ù‚ Ø§ÙˆÙ„ØªÛŒÙ…ÛŒØª Ø®ÙˆØ´ Ø¢Ù…Ø¯ÛŒØ¯",
        "status": "active",
        "version": "1.5.0",
        "ai_capabilities": {
            "openai_connected": has_openai,
            "memory_enabled": True,
            "persian_support": True,
            "fallback_mode": not has_openai
        },
        "endpoints": {
            "chat": "/api/chat",
            "health": "/api/health",
            "status": "/api/status",
            "clear_memory": "/api/clear-memory",
            "docs": "/api/docs"
        }
    }

@app.get("/api/health")
async def health_check():
    """Ø¨Ø±Ø±Ø³ÛŒ Ø³Ù„Ø§Ù…Øª"""
    has_openai = bool(client)
    return {
        "status": "healthy",
        "timestamp": __import__("datetime").datetime.now().isoformat(),
        "openai_status": "connected" if has_openai else "disconnected",
        "active_conversations": len(conversation_memory),
        "memory_usage": f"{len(str(conversation_memory)) / 1024:.2f} KB"
    }

@app.post("/api/chat")
async def chat_endpoint(request: Request):
    """Ù¾Ø§ÛŒØ§Ù†Ù‡ Ù…Ú©Ø§Ù„Ù…Ù‡ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ"""
    try:
        data = await request.json()
        message = data.get("message", "").strip()
        session_id = data.get("session_id", "default")
        
        if not message:
            raise HTTPException(status_code=400, detail="Ù¾ÛŒØ§Ù… Ù†Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø®Ø§Ù„ÛŒ Ø¨Ø§Ø´Ø¯")
        
        logger.info(f"ğŸ’¬ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ú†Øª: session={session_id}, length={len(message)}")
        
        # Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø­Ø§ÙØ¸Ù‡ Ù‚Ø¯ÛŒÙ…ÛŒ
        cleanup_old_memory()
        
        # Ø§Ù†Ø¯Ø§Ø²Ù‡â€ŒÚ¯ÛŒØ±ÛŒ Ø²Ù…Ø§Ù† Ù¾Ø§Ø³Ø®
        import time
        start_time = time.time()
        
        # Ø¯Ø±ÛŒØ§ÙØª Ù¾Ø§Ø³Ø® Ø§Ø² AI
        response = await call_openai(message, session_id)
        
        response_time = (time.time() - start_time) * 1000
        
        return {
            "success": True,
            "response": response,
            "session_id": session_id,
            "response_time": f"{response_time:.0f}ms",
            "model": "openai-gpt" if client else "fallback-model",
            "memory_size": len(conversation_memory.get(session_id, {}).get("history", [])) if session_id in conversation_memory else 0
        }
        
    except json.JSONDecodeError:
        raise HTTPException(status_code=400, detail="ÙØ±Ù…Øª JSON Ù†Ø§Ù…Ø¹ØªØ¨Ø±")
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ú†Øª: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Ø®Ø·Ø§ÛŒ Ø¯Ø§Ø®Ù„ÛŒ: {str(e)}")

@app.get("/api/status")
async def get_status():
    """ÙˆØ¶Ø¹ÛŒØª Ø³ÛŒØ³ØªÙ…"""
    import platform
    import sys
    
    return {
        "system": {
            "python_version": sys.version,
            "platform": platform.platform(),
            "api_version": "1.5.0"
        },
        "ai": {
            "openai_configured": bool(OPENAI_API_KEY),
            "openai_connected": bool(client),
            "fallback_active": not bool(client)
        },
        "memory": {
            "active_sessions": len(conversation_memory),
            "total_messages": sum(len(m["history"]) for m in conversation_memory.values())
        }
    }

@app.post("/api/clear-memory")
async def clear_memory(session_id: Optional[str] = None):
    """Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø­Ø§ÙØ¸Ù‡"""
    if session_id:
        if session_id in conversation_memory:
            del conversation_memory[session_id]
            return {"success": True, "message": f"Ø­Ø§ÙØ¸Ù‡ session {session_id} Ù¾Ø§Ú© Ø´Ø¯"}
        else:
            raise HTTPException(status_code=404, detail="Session ÛŒØ§ÙØª Ù†Ø´Ø¯")
    else:
        conversation_memory.clear()
        return {"success": True, "message": "ØªÙ…Ø§Ù…ÛŒ Ø­Ø§ÙØ¸Ù‡â€ŒÙ‡Ø§ Ù¾Ø§Ú© Ø´Ø¯"}

@app.get("/api/test-openai")
async def test_openai():
    """ØªØ³Øª Ø§ØªØµØ§Ù„ Ø¨Ù‡ OpenAI"""
    if not client:
        return {
            "success": False,
            "message": "OpenAI API Key ØªÙ†Ø¸ÛŒÙ… Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª",
            "instruction": "Ù„Ø·ÙØ§Ù‹ OPENAI_API_KEY Ø±Ø§ Ø¯Ø± Environment Variables ØªÙ†Ø¸ÛŒÙ… Ú©Ù†ÛŒØ¯"
        }
    
    try:
        test_prompt = "Ø³Ù„Ø§Ù…! Ù„Ø·ÙØ§Ù‹ ÛŒÚ© Ø¬Ù…Ù„Ù‡ Ú©ÙˆØªØ§Ù‡ ÙØ§Ø±Ø³ÛŒ Ø¨Ú¯Ùˆ."
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": test_prompt}],
            max_tokens=50
        )
        
        return {
            "success": True,
            "message": "OpenAI API Ù…ØªØµÙ„ Ø§Ø³Øª",
            "test_response": response.choices[0].message.content,
            "model": response.model,
            "usage": {
                "prompt_tokens": response.usage.prompt_tokens,
                "completion_tokens": response.usage.completion_tokens
            }
        }
        
    except Exception as e:
        return {
            "success": False,
            "message": f"Ø®Ø·Ø§ Ø¯Ø± Ø§ØªØµØ§Ù„ Ø¨Ù‡ OpenAI: {str(e)}",
            "error_type": type(e).__name__
        }

# ==================== Ù…Ø³ØªÙ†Ø¯Ø§Øª ====================

SWAGGER_UI_HTML = """
<!DOCTYPE html>
<html lang="fa" dir="rtl">
<head>
    <meta charset="UTF-8">
    <title>Ù†Ø§Ø·Ù‚ Ø§ÙˆÙ„ØªÛŒÙ…ÛŒØª - Ù…Ø³ØªÙ†Ø¯Ø§Øª API</title>
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/swagger-ui-dist@5/swagger-ui.css">
    <style>
        * { font-family: 'Vazirmatn', sans-serif !important; }
        body { margin: 0; padding: 20px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); }
        .container { max-width: 1200px; margin: 0 auto; background: white; padding: 30px; border-radius: 15px; box-shadow: 0 20px 40px rgba(0,0,0,0.1); }
        h1 { color: #333; text-align: center; margin-bottom: 30px; background: linear-gradient(90deg, #667eea, #764ba2); -webkit-background-clip: text; -webkit-text-fill-color: transparent; }
        .info-box { background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%); color: white; padding: 20px; border-radius: 10px; margin-bottom: 25px; }
        .status { padding: 10px 20px; border-radius: 20px; font-weight: bold; display: inline-block; margin: 10px 0; }
        .online { background: #2ecc71; color: white; }
        .offline { background: #e74c3c; color: white; }
    </style>
</head>
<body>
    <div class="container">
        <h1>ğŸ§  Ù†Ø§Ø·Ù‚ Ø§ÙˆÙ„ØªÛŒÙ…ÛŒØª - Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ</h1>
        
        <div class="info-box">
            <h2>ğŸ”„ ÙˆØ¶Ø¹ÛŒØª Ø³ÛŒØ³ØªÙ…</h2>
            <div id="status">Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø±Ø±Ø³ÛŒ...</div>
            <p><strong>Ø¢Ø¯Ø±Ø³ Ù¾Ø§ÛŒÙ‡:</strong> <code>https://natiq-ultimate.vercel.app/api</code></p>
            <p><strong>ÙˆØ±Ú˜Ù†:</strong> 1.5.0</p>
        </div>
        
        <h2>ğŸ“¡ EndpointÙ‡Ø§ÛŒ ÙØ¹Ø§Ù„</h2>
        <div id="swagger-ui"></div>
    </div>
    
    <script src="https://cdn.jsdelivr.net/npm/swagger-ui-dist@5/swagger-ui-bundle.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/swagger-ui-dist@5/swagger-ui-standalone-preset.js"></script>
    <script>
    window.onload = async function() {
        // Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¶Ø¹ÛŒØª
        try {
            const status = await fetch('/api/status').then(r => r.json());
            const statusDiv = document.getElementById('status');
            if (status.ai.openai_connected) {
                statusDiv.innerHTML = '<span class="status online">âœ… OpenAI Ù…ØªØµÙ„</span>';
            } else {
                statusDiv.innerHTML = '<span class="status offline">âš ï¸ Ø­Ø§Ù„Øª Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† ÙØ¹Ø§Ù„</span><br><small>Ø¨Ø±Ø§ÛŒ Ø§ØªØµØ§Ù„ Ø¨Ù‡ OpenAIØŒ OPENAI_API_KEY Ø±Ø§ ØªÙ†Ø¸ÛŒÙ… Ú©Ù†ÛŒØ¯</small>';
            }
        } catch (e) {
            console.error(e);
        }
        
        // Swagger UI
        const ui = SwaggerUIBundle({
            url: "/api/openapi.json",
            dom_id: '#swagger-ui',
            presets: [
                SwaggerUIBundle.presets.apis,
                SwaggerUIStandalonePreset
            ],
            layout: "StandaloneLayout",
            deepLinking: true,
            displayRequestDuration: true,
            docExpansion: 'list'
        });
        
        window.ui = ui;
    };
    </script>
</body>
</html>
"""

@app.get("/api/docs")
async def get_api_docs():
    """Ù…Ø³ØªÙ†Ø¯Ø§Øª Swagger UI"""
    return HTMLResponse(content=SWAGGER_UI_HTML, status_code=200)

@app.get("/api/openapi.json")
async def get_openapi_spec():
    """OpenAPI Specification"""
    return {
        "openapi": "3.0.0",
        "info": {
            "title": "Natiq Ultimate AI API",
            "description": "Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ ÙØ§Ø±Ø³ÛŒ Ø¨Ø§ Ù‚Ø§Ø¨Ù„ÛŒØª Ù…Ú©Ø§Ù„Ù…Ù‡ Ùˆ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ØªÙ†",
            "version": "1.5.0",
            "contact": {
                "name": "Ù†Ø§Ø·Ù‚ Ø§ÙˆÙ„ØªÛŒÙ…ÛŒØª",
                "url": "https://natiq-ultimate.vercel.app"
            }
        },
        "servers": [
            {
                "url": "https://natiq-ultimate.vercel.app/api",
                "description": "Ø³Ø±ÙˆØ± ØªÙˆÙ„ÛŒØ¯"
            }
        ],
        "paths": {
            "/chat": {
                "post": {
                    "summary": "Ù…Ú©Ø§Ù„Ù…Ù‡ Ø¨Ø§ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ",
                    "description": "Ø§Ø±Ø³Ø§Ù„ Ù¾ÛŒØ§Ù… Ùˆ Ø¯Ø±ÛŒØ§ÙØª Ù¾Ø§Ø³Ø® Ø§Ø² AI",
                    "requestBody": {
                        "required": true,
                        "content": {
                            "application/json": {
                                "schema": {
                                    "type": "object",
                                    "properties": {
                                        "message": {
                                            "type": "string",
                                            "description": "Ù¾ÛŒØ§Ù… Ú©Ø§Ø±Ø¨Ø±"
                                        },
                                        "session_id": {
                                            "type": "string",
                                            "description": "Ø´Ù†Ø§Ø³Ù‡ Ø¬Ù„Ø³Ù‡ (Ø§Ø®ØªÛŒØ§Ø±ÛŒ)"
                                        }
                                    }
                                }
                            }
                        }
                    },
                    "responses": {
                        "200": {
                            "description": "Ù¾Ø§Ø³Ø® Ù…ÙˆÙÙ‚"
                        },
                        "400": {
                            "description": "Ù¾ÛŒØ§Ù… Ø®Ø§Ù„ÛŒ ÛŒØ§ Ù†Ø§Ù…Ø¹ØªØ¨Ø±"
                        }
                    }
                }
            }
        }
    }

# ==================== Ù…Ø¯ÛŒØ±ÛŒØª Ø®Ø·Ø§ ====================

@app.exception_handler(404)
async def not_found_handler(request: Request, exc: Exception):
    return JSONResponse(
        status_code=404,
        content={
            "success": False,
            "error": "Ù…Ø³ÛŒØ± ÛŒØ§ÙØª Ù†Ø´Ø¯",
            "path": str(request.url.path),
            "suggestion": "Ø§Ø² endpoint /api/chat Ø¨Ø±Ø§ÛŒ Ù…Ú©Ø§Ù„Ù…Ù‡ Ø¨Ø§ AI Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯"
        }
    )

# Middleware Ø¨Ø±Ø§ÛŒ Ù„Ø§Ú¯â€ŒÚ¯ÛŒØ±ÛŒ
@app.middleware("http")
async def log_requests(request: Request, call_next):
    import time
    start_time = time.time()
    
    response = await call_next(request)
    
    process_time = (time.time() - start_time) * 1000
    logger.info(f"{request.method} {request.url.path} - {response.status_code} - {process_time:.0f}ms")
    
    return response

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "app:app",
        host="0.0.0.0",
        port=int(os.getenv("PORT", "8000")),
        reload=True
    )
