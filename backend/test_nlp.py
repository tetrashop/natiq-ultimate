#!/usr/bin/env python3
"""
ØªØ³Øª Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù¾Ø§ÛŒÙ‡ NLP
"""

import sys
from pathlib import Path

# Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…Ø³ÛŒØ± src Ø¨Ù‡ sys.path
sys.path.append(str(Path(__file__).parent))

try:
    from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM
    import torch
    print("âœ… Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯Ù†Ø¯")
except ImportError as e:
    print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± ÙˆØ§Ø±Ø¯Ø³Ø§Ø²ÛŒ Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§: {e}")
    print("Ù„Ø·ÙØ§ Ø¯Ø³ØªÙˆØ± Ø²ÛŒØ± Ø±Ø§ Ø§Ø¬Ø±Ø§ Ú©Ù†ÛŒØ¯:")
    print("pip install transformers torch")
    sys.exit(1)

def test_simple_chat():
    """ØªØ³Øª Ø³Ø§Ø¯Ù‡ Ú¯ÙØªÚ¯Ùˆ"""
    print("\nğŸ§ª ØªØ³Øª Ø³ÛŒØ³ØªÙ… Ù¾Ø±Ø³Ø´ Ùˆ Ù¾Ø§Ø³Ø®")
    print("-" * 40)
    
    # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ÛŒÚ© Ù…Ø¯Ù„ Ú©ÙˆÚ†Ú© Ùˆ Ø³Ø±ÛŒØ¹
    model_name = "google/flan-t5-small"  # Ù…Ø¯Ù„ Ø³Ø¨Ú© Ùˆ Ø³Ø±ÛŒØ¹
    
    print(f"ğŸ“¥ Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„: {model_name}")
    print("Ù„Ø·ÙØ§ Ù…Ù†ØªØ¸Ø± Ø¨Ù…Ø§Ù†ÛŒØ¯ (Ù…Ù…Ú©Ù† Ø§Ø³Øª Ú†Ù†Ø¯ Ø¯Ù‚ÛŒÙ‚Ù‡ Ø·ÙˆÙ„ Ø¨Ú©Ø´Ø¯)...")
    
    try:
        # Ø³Ø§Ø®Øª pipeline Ø¨Ø±Ø§ÛŒ Ù…ØªÙ† Ø¨Ù‡ Ù…ØªÙ†
        qa_pipeline = pipeline(
            "text2text-generation",
            model=model_name,
            device=-1  # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² CPU
        )
        
        print("âœ… Ù…Ø¯Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯!")
        
        while True:
            print("\n" + "=" * 50)
            question = input("Ø³ÙˆØ§Ù„ Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ù¾Ø±Ø³ÛŒØ¯ (ÛŒØ§ 'Ø®Ø±ÙˆØ¬' Ø¨Ø±Ø§ÛŒ Ù¾Ø§ÛŒØ§Ù†): ")
            
            if question.lower() in ['Ø®Ø±ÙˆØ¬', 'exit', 'quit']:
                print("ğŸ‘‹ Ø®Ø¯Ø§Ø­Ø§ÙØ¸!")
                break
            
            if not question.strip():
                print("âš ï¸ Ù„Ø·ÙØ§ ÛŒÚ© Ø³ÙˆØ§Ù„ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯")
                continue
            
            print(f"\nğŸ“ Ø´Ù…Ø§ Ù¾Ø±Ø³ÛŒØ¯ÛŒØ¯: {question}")
            print("ğŸ¤” Ø¯Ø± Ø­Ø§Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´...")
            
            try:
                # ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø®
                result = qa_pipeline(
                    question,
                    max_length=100,
                    do_sample=True,
                    temperature=0.7
                )
                
                answer = result[0]['generated_text']
                print(f"ğŸ¤– Ù¾Ø§Ø³Ø®: {answer}")
                
            except Exception as e:
                print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´: {e}")
                print("Ø´Ø§ÛŒØ¯ Ù…Ø¯Ù„ Ø¯Ø± Ø­Ø§Ù„ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø§Ø³Øª...")
        
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„: {e}")
        print("\nØ±Ø§Ù‡â€ŒØ­Ù„â€ŒÙ‡Ø§ÛŒ Ù…Ù…Ú©Ù†:")
        print("1. Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ø§ÛŒÙ†ØªØ±Ù†Øª")
        print("2. Ù†ØµØ¨ Ù…Ø¬Ø¯Ø¯ transformers: pip install --upgrade transformers")
        print("3. Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¯Ù„ Ú©ÙˆÚ†Ú©ØªØ±")

if __name__ == "__main__":
    print("ğŸš€ Ø´Ø±ÙˆØ¹ ØªØ³Øª natiq-ultimate")
    test_simple_chat()
